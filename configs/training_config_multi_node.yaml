Model:
  predictor: "autonomous_motion"
  hidden_dim: 256
  num_heads: 8
  num_layers: 6
  dropout: 0.1
  activation: "gelu"
  num_historical_steps: 11
  num_future_steps: 80
  num_total_steps: 91
  
  advanced_features:
    use_lane_tokens: true
    use_relational_attention: true
    lane_tokens:
      count: 100
      bucket_size: 100
      embedding_dim: 64
    relational_attention:
      num_heads: 8
      head_dim: 16
      rel_feature_dim: 5
      bias_dim: 8

  decoder:
    hidden_dim: 256
    num_heads: 8
    num_layers: 6
    dropout: 0.1
    token_size: 1000
    use_lane_tokens: true
    use_relational_attention: true
    lane_token_embedding_dim: 64

Dataset:
  root: "." 
  name: "waymo_motion"
  batch_size: 16 
  num_workers: 4 
  shuffle: true
  pin_memory: true
  persistent_workers: true
  num_historical_steps: 11
  num_future_steps: 80
  num_total_steps: 91

Trainer:
  max_epochs: 100
  accelerator: "gpu"
  devices: 4 
  strategy: "ddp" 
  precision: "16-mixed" 
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  log_every_n_steps: 100
  val_check_interval: 1000 
  limit_val_batches: 0.1 
  enable_checkpointing: true
  enable_progress_bar: true
  enable_model_summary: true
  
  num_nodes: 1
  sync_batchnorm: true
  find_unused_parameters: false
  
  callbacks:
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        dirpath: null 
        filename: "advanced_features_step_{step:06d}"
        monitor: "val_loss"
        mode: "min"
        save_top_k: 3
        save_last: true
        every_n_train_steps: 1000
        save_on_train_epoch_end: false

Logger:
  class_path: pytorch_lightning.loggers.TensorBoardLogger
  init_args:
    save_dir: "logs"
    name: "adp_multi_node"
    version: null
    log_graph: false
    default_hp_metric: true

Optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.001
    weight_decay: 0.01
    betas: [0.9, 0.999]
    eps: 1e-8

Scheduler:
  class_path: torch.optim.lr_scheduler.CosineAnnealingLR
  init_args:
    T_max: 100
    eta_min: 1e-6

Loss:
  class_path: torch.nn.CrossEntropyLoss
  init_args:
    reduction: "mean"
    label_smoothing: 0.1

time_info: &time_info
  num_historical_steps: 11
  num_future_steps: 80
  num_total_steps: 91

